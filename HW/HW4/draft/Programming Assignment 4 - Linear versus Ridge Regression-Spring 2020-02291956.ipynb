{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment 4 - Linear versus Ridge Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your friend Bob just moved to Boston. He is a real estate agent who is trying to evaluate the prices of houses in the Boston area. He has been using a linear regression model but he wonders if he can improve his accuracy on predicting the prices for new houses. He comes to you for help as he knows that you're an expert in machine learning. \n",
    "\n",
    "As a pro, you suggest doing a *polynomial transformation*  to create a more flexible model, and performing ridge regression since having so many features compared to data points increases the variance. \n",
    "\n",
    "Bob, however, being a skeptic isn't convinced. He wants you to write a program that illustrates the difference in training and test costs for both linear and ridge regression on the same dataset. Being a good friend, you oblige and hence this assignment :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you are to explore the effects of ridge regression.  We will use a dataset that is part of the sklearn.dataset package.  Learn more at https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1:  Getting, understanding, and preprocessing the dataset\n",
    "\n",
    "We first import the standard libaries and some libraries that will help us scale the data and perform some \"feature engineering\" by transforming the data into $\\Phi_2({\\bf x})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.linear_model\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the boston dataset from sklearn\n",
    "boston_data = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of features is:  13\n",
      "The features:  ['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n",
      "The number of exampels in our dataset:  506\n",
      "[[6.3200e-03 1.8000e+01 2.3100e+00 0.0000e+00 5.3800e-01 6.5750e+00\n",
      "  6.5200e+01 4.0900e+00 1.0000e+00 2.9600e+02 1.5300e+01 3.9690e+02\n",
      "  4.9800e+00]\n",
      " [2.7310e-02 0.0000e+00 7.0700e+00 0.0000e+00 4.6900e-01 6.4210e+00\n",
      "  7.8900e+01 4.9671e+00 2.0000e+00 2.4200e+02 1.7800e+01 3.9690e+02\n",
      "  9.1400e+00]]\n"
     ]
    }
   ],
   "source": [
    "#  Create X and Y variables - X holding the .data and Y holding .target \n",
    "X = boston_data.data\n",
    "y = boston_data.target\n",
    "\n",
    "#  Reshape Y to be a rank 2 matrix \n",
    "y = y.reshape(X.shape[0], 1)\n",
    "\n",
    "# Proprocesing by adding a column of 1's to the front of X\n",
    "# one_col = np.ones((X.shape[0],1))\n",
    "# X = np.hstack((one_col, X))\n",
    "\n",
    "# Observe the number of features and the number of labels\n",
    "print('The number of features is: ', X.shape[1])\n",
    "# Printing out the features\n",
    "print('The features: ', boston_data.feature_names)\n",
    "# The number of examples\n",
    "print('The number of exampels in our dataset: ', X.shape[0])\n",
    "#Observing the first 2 rows of the data\n",
    "print(X[0:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also create polynomial features for the dataset to test linear and ridge regression on data with degree = 1 and data with degree = 2. Feel free to increase the # of degress and see what effect it has on the training and test error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PolynomialFeatures object with degree = 2. \n",
    "# Transform X and save it into X_2. Simply copy Y into Y_2 \n",
    "# Note: PolynomialFeatures creates a column of ones as the first feature\n",
    "# poly = PolynomialFeatures(degree=2, include_bias=True, interaction_only=False)\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)\n",
    "X_2 = poly.fit_transform(X)\n",
    "y_2 = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 104)\n",
      "(506, 1)\n"
     ]
    }
   ],
   "source": [
    "# the shape of X_2 and Y_2 - should be (506, 105) and (506, 1) respectively\n",
    "print(X_2.shape)\n",
    "print(y_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Define the get_coeff_ridge_normaleq function. Use the normal equation method.\n",
    "# TODO - Return w values\n",
    "\n",
    "def get_coeff_ridge_normaleq(X_train, y_train, alpha):\n",
    "    # use np.linalg.pinv(a)\n",
    "    #### TO-DO #####\n",
    "    N = X_train.shape[0]\n",
    "    identity = np.eye(X_train.shape[1])\n",
    "    identity[0][0] = 0.0\n",
    "    # w = np.linalg.pinv((X_train).T.dot(X_train) + N*alpha*identity).dot((X_train).T).dot(y_train)\n",
    "    w = np.linalg.pinv((X_train).T.dot(X_train) + alpha*identity).dot((X_train).T).dot(y_train)\n",
    "    ##############\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Define the evaluate_err_ridge function.\n",
    "# TODO - Return the train_error and test_error values\n",
    "\n",
    "\n",
    "def evaluate_err(X_train, X_test, y_train, y_test, w): \n",
    "    #### TO-DO #####\n",
    "    N_train = X_train.shape[0]\n",
    "    train_error = (1 / N_train) * (np.transpose(X_train.dot(w)-y_train)).dot(X_train.dot(w)-y_train)\n",
    "    N_test = X_test.shape[0] \n",
    "    test_error = (1 / N_test) * (np.transpose(X_test.dot(w)-y_test)).dot(X_test.dot(w)-y_test)\n",
    "    ##############\n",
    "    return train_error[0][0], test_error[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Finish writting the k_fold_cross_validation function. \n",
    "# TODO - Returns the average training error and average test error from the k-fold cross validation\n",
    "# use Sklearns K-Folds cross-validator: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
    "\n",
    "def k_fold_cross_validation(k, X, y, alpha):\n",
    "    kf = KFold(n_splits=k, random_state=21, shuffle=True)\n",
    "    total_E_val_test = 0\n",
    "    total_E_val_train = 0\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # scaling the data matrix (except for for the first column of ones)\n",
    "#         scaler = preprocessing.StandardScaler().fit(X_train[:,1:(X_train.shape[1]+1)])\n",
    "#         X_train[:,1:(X_train.shape[1]+1)] = scaler.transform(X_train[:,1:(X_train.shape[1]+1)])\n",
    "#         X_test[:,1:(X_train.shape[1]+1)] = scaler.transform(X_test[:,1:(X_train.shape[1]+1)])\n",
    "        scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "    \n",
    "        # Proprocesing by adding a column of 1's to the front of X\n",
    "        one_col_train = np.ones((X_train.shape[0],1))\n",
    "        X_train = np.hstack((one_col_train, X_train))\n",
    "        one_col_test = np.ones((X_test.shape[0],1))\n",
    "        X_test = np.hstack((one_col_test, X_test))\n",
    "        \n",
    "        # determine the training error and the test error\n",
    "        #### TO-DO #####\n",
    "        w = get_coeff_ridge_normaleq(X_train, y_train, alpha)\n",
    "        (train_error, test_error) = evaluate_err(X_train, X_test, y_train, y_test, w)\n",
    "        total_E_val_train += train_error\n",
    "        total_E_val_test += test_error\n",
    "        \n",
    "    E_val_train = total_E_val_train / k\n",
    "    E_val_test = total_E_val_test / k\n",
    "       ##############\n",
    "    return  E_val_test, E_val_train\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the value of k (k-fold cross validation)\n",
    "k = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Experiment with a linear regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the best results for question 1\n",
    "degree = 1\n",
    "result_1 = []  # [alpha, E_val_test, E_val_train, degree]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for test set: 23.636068605428214\n",
      "MSE for training set: 21.806183575851065\n"
     ]
    }
   ],
   "source": [
    "# 1-a  fit a linear model\n",
    "alpha = 0.\n",
    "E_val_test, E_val_train = k_fold_cross_validation(k, X, y, alpha)\n",
    "print(\"MSE for test set: {}\".format(E_val_test))\n",
    "print(\"MSE for training set: {}\".format(E_val_train))\n",
    "result_1 += [[0., E_val_test, E_val_train, degree]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best alpha is 1.4962356560944334, with test error: 23.633077307882157\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>MSE_test</th>\n",
       "      <th>MSE_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.023293</td>\n",
       "      <td>23.633413</td>\n",
       "      <td>21.807512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.237371</td>\n",
       "      <td>23.633196</td>\n",
       "      <td>21.808105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.496236</td>\n",
       "      <td>23.633077</td>\n",
       "      <td>21.808958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.809256</td>\n",
       "      <td>23.633137</td>\n",
       "      <td>21.810179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.187762</td>\n",
       "      <td>23.633492</td>\n",
       "      <td>21.811920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.645453</td>\n",
       "      <td>23.634312</td>\n",
       "      <td>21.814392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.198895</td>\n",
       "      <td>23.635837</td>\n",
       "      <td>21.817882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.868121</td>\n",
       "      <td>23.638400</td>\n",
       "      <td>21.822778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>4.677351</td>\n",
       "      <td>23.642454</td>\n",
       "      <td>21.829599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>5.655878</td>\n",
       "      <td>23.648603</td>\n",
       "      <td>21.839024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>6.839116</td>\n",
       "      <td>23.657632</td>\n",
       "      <td>21.851931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>8.269895</td>\n",
       "      <td>23.670544</td>\n",
       "      <td>21.869429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>23.688583</td>\n",
       "      <td>21.892901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        alpha   MSE_test  MSE_train\n",
       "0    1.023293  23.633413  21.807512\n",
       "1    1.237371  23.633196  21.808105\n",
       "2    1.496236  23.633077  21.808958\n",
       "3    1.809256  23.633137  21.810179\n",
       "4    2.187762  23.633492  21.811920\n",
       "5    2.645453  23.634312  21.814392\n",
       "6    3.198895  23.635837  21.817882\n",
       "7    3.868121  23.638400  21.822778\n",
       "8    4.677351  23.642454  21.829599\n",
       "9    5.655878  23.648603  21.839024\n",
       "10   6.839116  23.657632  21.851931\n",
       "11   8.269895  23.670544  21.869429\n",
       "12  10.000000  23.688583  21.892901"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1-b  ﬁt a ridge regression model \n",
    "Alpha = np.logspace(.01, 1, num=13)\n",
    "res1 = []\n",
    "for alpha in Alpha:\n",
    "    E_val_test, E_val_train = k_fold_cross_validation(k, X, y, alpha)\n",
    "    res1 += [[alpha, E_val_test, E_val_train]]\n",
    "    result_1 += [[alpha, E_val_test, E_val_train, degree]]\n",
    "\n",
    "res1_DF = pd.DataFrame(res1, columns=['alpha', 'MSE_test','MSE_train'])\n",
    "res1_sorted = res1_DF.sort_values(by=['MSE_test'])\n",
    "print(\"The best alpha is {}, with test error: {}\".format(res1_sorted.iloc[0][0], res1_sorted.iloc[0][1]))\n",
    "res1_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best alpha is 1.023292992280754, with test error: 23.633413370890672\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>MSE_test</th>\n",
       "      <th>MSE_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>23.636068</td>\n",
       "      <td>21.806184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>23.636068</td>\n",
       "      <td>21.806184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>23.636068</td>\n",
       "      <td>21.806184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>23.636067</td>\n",
       "      <td>21.806184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>23.636067</td>\n",
       "      <td>21.806184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000685</td>\n",
       "      <td>23.636066</td>\n",
       "      <td>21.806184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.001006</td>\n",
       "      <td>23.636065</td>\n",
       "      <td>21.806184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.001478</td>\n",
       "      <td>23.636063</td>\n",
       "      <td>21.806184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.002171</td>\n",
       "      <td>23.636060</td>\n",
       "      <td>21.806184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.003190</td>\n",
       "      <td>23.636056</td>\n",
       "      <td>21.806184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.004686</td>\n",
       "      <td>23.636050</td>\n",
       "      <td>21.806184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.006885</td>\n",
       "      <td>23.636041</td>\n",
       "      <td>21.806184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.010116</td>\n",
       "      <td>23.636028</td>\n",
       "      <td>21.806184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.014862</td>\n",
       "      <td>23.636010</td>\n",
       "      <td>21.806184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.021836</td>\n",
       "      <td>23.635982</td>\n",
       "      <td>21.806184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.032081</td>\n",
       "      <td>23.635942</td>\n",
       "      <td>21.806185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.047134</td>\n",
       "      <td>23.635884</td>\n",
       "      <td>21.806187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.069250</td>\n",
       "      <td>23.635799</td>\n",
       "      <td>21.806190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.101742</td>\n",
       "      <td>23.635678</td>\n",
       "      <td>21.806197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.149480</td>\n",
       "      <td>23.635504</td>\n",
       "      <td>21.806213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.219617</td>\n",
       "      <td>23.635261</td>\n",
       "      <td>21.806247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.322664</td>\n",
       "      <td>23.634929</td>\n",
       "      <td>21.806320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.474060</td>\n",
       "      <td>23.634493</td>\n",
       "      <td>21.806477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.696493</td>\n",
       "      <td>23.633963</td>\n",
       "      <td>21.806809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.023293</td>\n",
       "      <td>23.633413</td>\n",
       "      <td>21.807512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       alpha   MSE_test  MSE_train\n",
       "0   0.000100  23.636068  21.806184\n",
       "1   0.000147  23.636068  21.806184\n",
       "2   0.000216  23.636068  21.806184\n",
       "3   0.000317  23.636067  21.806184\n",
       "4   0.000466  23.636067  21.806184\n",
       "5   0.000685  23.636066  21.806184\n",
       "6   0.001006  23.636065  21.806184\n",
       "7   0.001478  23.636063  21.806184\n",
       "8   0.002171  23.636060  21.806184\n",
       "9   0.003190  23.636056  21.806184\n",
       "10  0.004686  23.636050  21.806184\n",
       "11  0.006885  23.636041  21.806184\n",
       "12  0.010116  23.636028  21.806184\n",
       "13  0.014862  23.636010  21.806184\n",
       "14  0.021836  23.635982  21.806184\n",
       "15  0.032081  23.635942  21.806185\n",
       "16  0.047134  23.635884  21.806187\n",
       "17  0.069250  23.635799  21.806190\n",
       "18  0.101742  23.635678  21.806197\n",
       "19  0.149480  23.635504  21.806213\n",
       "20  0.219617  23.635261  21.806247\n",
       "21  0.322664  23.634929  21.806320\n",
       "22  0.474060  23.634493  21.806477\n",
       "23  0.696493  23.633963  21.806809\n",
       "24  1.023293  23.633413  21.807512"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1-c  ﬁt a ridge regression model with different alpha\n",
    "Alpha = np.logspace(-4, 0.01, num=25)\n",
    "res1 = []\n",
    "for alpha in Alpha:\n",
    "    E_val_test, E_val_train = k_fold_cross_validation(k, X, y, alpha)\n",
    "    res1 += [[alpha, E_val_test, E_val_train]]\n",
    "    result_1 += [[alpha, E_val_test, E_val_train, degree]]\n",
    "    \n",
    "res1_DF = pd.DataFrame(res1, columns=['alpha', 'MSE_test','MSE_train'])\n",
    "res1_sorted = res1_DF.sort_values(by=['MSE_test'])\n",
    "print(\"The best alpha is {}, with test error: {}\".format(res1_sorted.iloc[0][0], res1_sorted.iloc[0][1]))\n",
    "res1_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.4962356560944334, 23.633077307882157, 21.808958002887373, 1]\n",
      "[1.8092559102538208, 23.633136915516598, 21.810178867543744, 1]\n",
      "[1.2373711899353526, 23.633195858169987, 21.80810541222859, 1]\n",
      "[1.023292992280754, 23.633413370890672, 21.807512026567768, 1]\n",
      "[1.023292992280754, 23.633413370890672, 21.807512026567768, 1]\n"
     ]
    }
   ],
   "source": [
    "# Get the best alpha when degree = 1\n",
    "\n",
    "result_1.sort(key=lambda x: x[1])\n",
    "for i in range(5):\n",
    "    print((result_1[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Create a polynomial transformation of degree 2 on the features of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the best results for question 2\n",
    "degree = 2\n",
    "result_2 = []  # [alpha, E_val_test, E_val_train, degree]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for test set: 11.85496823342098\n",
      "MSE for training set: 5.80882081601246\n"
     ]
    }
   ],
   "source": [
    "# 2-a  polynomial transformation with alpha=0\n",
    "alpha = 0.\n",
    "E_val_test, E_val_train = k_fold_cross_validation(k, X_2, y_2, alpha)\n",
    "\n",
    "print(\"MSE for test set: {}\".format(E_val_test))\n",
    "print(\"MSE for training set: {}\".format(E_val_train))\n",
    "result_2 += [[0.0, E_val_test, E_val_train, degree]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best alpha is 1.023292992280754, with test error: 11.757217352780899\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>MSE_test</th>\n",
       "      <th>MSE_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.023293</td>\n",
       "      <td>11.757217</td>\n",
       "      <td>7.429053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.237371</td>\n",
       "      <td>11.837287</td>\n",
       "      <td>7.574286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.496236</td>\n",
       "      <td>11.924501</td>\n",
       "      <td>7.728998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.809256</td>\n",
       "      <td>12.019239</td>\n",
       "      <td>7.893864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.187762</td>\n",
       "      <td>12.122080</td>\n",
       "      <td>8.069629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.645453</td>\n",
       "      <td>12.233937</td>\n",
       "      <td>8.257179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.198895</td>\n",
       "      <td>12.356226</td>\n",
       "      <td>8.457641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.868121</td>\n",
       "      <td>12.491022</td>\n",
       "      <td>8.672500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>4.677351</td>\n",
       "      <td>12.641170</td>\n",
       "      <td>8.903711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>5.655878</td>\n",
       "      <td>12.810296</td>\n",
       "      <td>9.153772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>6.839116</td>\n",
       "      <td>13.002701</td>\n",
       "      <td>9.425708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>8.269895</td>\n",
       "      <td>13.223089</td>\n",
       "      <td>9.722948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>13.476138</td>\n",
       "      <td>10.049056</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        alpha   MSE_test  MSE_train\n",
       "0    1.023293  11.757217   7.429053\n",
       "1    1.237371  11.837287   7.574286\n",
       "2    1.496236  11.924501   7.728998\n",
       "3    1.809256  12.019239   7.893864\n",
       "4    2.187762  12.122080   8.069629\n",
       "5    2.645453  12.233937   8.257179\n",
       "6    3.198895  12.356226   8.457641\n",
       "7    3.868121  12.491022   8.672500\n",
       "8    4.677351  12.641170   8.903711\n",
       "9    5.655878  12.810296   9.153772\n",
       "10   6.839116  13.002701   9.425708\n",
       "11   8.269895  13.223089   9.722948\n",
       "12  10.000000  13.476138  10.049056"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2-b ﬁt a ridge regression model \n",
    "Alpha = np.logspace(.01, 1, num=13)\n",
    "res2 = []\n",
    "for alpha in Alpha:\n",
    "    E_val_test, E_val_train = k_fold_cross_validation(k, X_2, y_2, alpha)\n",
    "    res2 += [[alpha, E_val_test, E_val_train]]\n",
    "    result_2 += [[alpha, E_val_test, E_val_train, degree]]\n",
    "    \n",
    "res2_DF = pd.DataFrame(res2, columns=['alpha', 'MSE_test','MSE_train'])\n",
    "res2_sorted = res2_DF.sort_values(by=['MSE_test'])\n",
    "print(\"The best alpha is {}, with test error: {}\".format(res2_sorted.iloc[0][0], res2_sorted.iloc[0][1]))\n",
    "res2_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best alpha is 0.10174193661806046, with test error: 11.30448883656713\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>MSE_test</th>\n",
       "      <th>MSE_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>11.807933</td>\n",
       "      <td>5.810779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>11.798512</td>\n",
       "      <td>5.811494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>11.788001</td>\n",
       "      <td>5.812315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>11.776059</td>\n",
       "      <td>5.813278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>11.762324</td>\n",
       "      <td>5.814475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000685</td>\n",
       "      <td>11.746473</td>\n",
       "      <td>5.816060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.001006</td>\n",
       "      <td>11.728165</td>\n",
       "      <td>5.818225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.001478</td>\n",
       "      <td>11.706887</td>\n",
       "      <td>5.821194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.002171</td>\n",
       "      <td>11.681866</td>\n",
       "      <td>5.825274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.003190</td>\n",
       "      <td>11.652206</td>\n",
       "      <td>5.830975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.004686</td>\n",
       "      <td>11.617241</td>\n",
       "      <td>5.839182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.006885</td>\n",
       "      <td>11.576881</td>\n",
       "      <td>5.851280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.010116</td>\n",
       "      <td>11.531745</td>\n",
       "      <td>5.869158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.014862</td>\n",
       "      <td>11.483163</td>\n",
       "      <td>5.895107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.021836</td>\n",
       "      <td>11.433316</td>\n",
       "      <td>5.931718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.032081</td>\n",
       "      <td>11.385574</td>\n",
       "      <td>5.981843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.047134</td>\n",
       "      <td>11.344586</td>\n",
       "      <td>6.048568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.069250</td>\n",
       "      <td>11.315782</td>\n",
       "      <td>6.134987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.101742</td>\n",
       "      <td>11.304489</td>\n",
       "      <td>6.243750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.149480</td>\n",
       "      <td>11.315153</td>\n",
       "      <td>6.376584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.219617</td>\n",
       "      <td>11.350807</td>\n",
       "      <td>6.534194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.322664</td>\n",
       "      <td>11.412827</td>\n",
       "      <td>6.716811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.474060</td>\n",
       "      <td>11.501262</td>\n",
       "      <td>6.925172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.696493</td>\n",
       "      <td>11.615883</td>\n",
       "      <td>7.161365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.023293</td>\n",
       "      <td>11.757217</td>\n",
       "      <td>7.429053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       alpha   MSE_test  MSE_train\n",
       "0   0.000100  11.807933   5.810779\n",
       "1   0.000147  11.798512   5.811494\n",
       "2   0.000216  11.788001   5.812315\n",
       "3   0.000317  11.776059   5.813278\n",
       "4   0.000466  11.762324   5.814475\n",
       "5   0.000685  11.746473   5.816060\n",
       "6   0.001006  11.728165   5.818225\n",
       "7   0.001478  11.706887   5.821194\n",
       "8   0.002171  11.681866   5.825274\n",
       "9   0.003190  11.652206   5.830975\n",
       "10  0.004686  11.617241   5.839182\n",
       "11  0.006885  11.576881   5.851280\n",
       "12  0.010116  11.531745   5.869158\n",
       "13  0.014862  11.483163   5.895107\n",
       "14  0.021836  11.433316   5.931718\n",
       "15  0.032081  11.385574   5.981843\n",
       "16  0.047134  11.344586   6.048568\n",
       "17  0.069250  11.315782   6.134987\n",
       "18  0.101742  11.304489   6.243750\n",
       "19  0.149480  11.315153   6.376584\n",
       "20  0.219617  11.350807   6.534194\n",
       "21  0.322664  11.412827   6.716811\n",
       "22  0.474060  11.501262   6.925172\n",
       "23  0.696493  11.615883   7.161365\n",
       "24  1.023293  11.757217   7.429053"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2-c ﬁt a ridge regression model with different alpha\n",
    "Alpha = np.logspace(-4, 0.01, num=25)\n",
    "res2 = []\n",
    "for alpha in Alpha:\n",
    "    E_val_test, E_val_train = k_fold_cross_validation(k, X_2, y_2, alpha)\n",
    "    res2 += [[alpha, E_val_test, E_val_train]]\n",
    "    result_2 += [[alpha, E_val_test, E_val_train, degree]]\n",
    "    \n",
    "res2_DF = pd.DataFrame(res2, columns=['alpha', 'MSE_test','MSE_train'])\n",
    "res2_sorted = res2_DF.sort_values(by=['MSE_test'])\n",
    "print(\"The best alpha is {}, with test error: {}\".format(res2_sorted.iloc[0][0], res2_sorted.iloc[0][1]))\n",
    "res2_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10174193661806046, 11.30448883656713, 6.243750395468105, 2]\n",
      "[0.14948008403608803, 11.315152659190606, 6.3765838945702, 2]\n",
      "[0.06924950392919477, 11.315781521519693, 6.134987053244873, 2]\n",
      "[0.04713389536157405, 11.34458608343532, 6.048567941783648, 2]\n",
      "[0.21961736002054366, 11.350807089017739, 6.5341941661972225, 2]\n"
     ]
    }
   ],
   "source": [
    "# Get the best alpha when degree = 2\n",
    "\n",
    "result_2.sort(key=lambda x: x[1])\n",
    "for j in range(5):\n",
    "    print(result_2[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Answer the questions into a PDF ﬁle called proganswers.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3-a the estimated MSE for the diﬀerent values of λ\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# when degree = 1\n",
    "DF = pd.DataFrame(result_1, columns=['alpha', 'MSE_test','MSE_train','degree'])\n",
    "alpha_x = DF['alpha']\n",
    "mse_test = DF['MSE_test']\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(alpha_x, mse_test, 'ro')\n",
    "plt.title('d = 1')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('test_error')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "alpha_x = alpha_x[0:20]\n",
    "mse_test = mse_test[0:20]\n",
    "plt.plot(alpha_x, mse_test, 'ro')\n",
    "plt.title('(enlarged view)')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('test_error')\n",
    "\n",
    "# when degree = 2\n",
    "DF = pd.DataFrame(result_2, columns=['alpha', 'MSE_test','MSE_train','degree'])\n",
    "alpha_x = DF['alpha']\n",
    "mse_test = DF['MSE_test']\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(alpha_x, mse_test, 'go')\n",
    "plt.title('d = 2')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('test_error')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "alpha_x = alpha_x[0:7]\n",
    "mse_test = mse_test[0:7]\n",
    "plt.plot(alpha_x, mse_test, 'go')\n",
    "plt.title('d = 2(enlarged view)')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('test_error')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best alpha is 0.10174193661806046, with the best degree: 2\n"
     ]
    }
   ],
   "source": [
    "# 3-b choose which model is better\n",
    "model_best = min(result_1[0], result_2[0], key=lambda x: x[1])\n",
    "alpha_best = model_best[0]\n",
    "d_best = model_best[-1]\n",
    "print(\"The best alpha is {}, with the best degree: {}\".format(alpha_best, d_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted price is: 220.36920388021673, when alpha = 0.10174193661806046\n"
     ]
    }
   ],
   "source": [
    "#  Reﬁt the parameters using all the data and predict the price \n",
    "X_single_test = np.array([[5, 0.5, 2, 0, 4, 8, 4, 6, 2, 2, 2, 4, 5.5]])\n",
    "# X_single_test = X_single_test.reshape(1, X_single_test.shape[0])\n",
    "X_all_data = X\n",
    "y_all_data = y\n",
    "\n",
    "if d_best == 2:\n",
    "    X_all_data = X_2\n",
    "    X_single_test = poly.fit_transform(X_single_test)\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_all_data)\n",
    "X_all_data = scaler.transform(X_all_data)\n",
    "X_single_test = scaler.transform(X_single_test)\n",
    "\n",
    "# y_best_mean = np.mean(y_best)\n",
    "# y_best -= y_best_mean\n",
    "\n",
    "# Proprocesing by adding a column of 1's to the front of X\n",
    "one_col_train = np.ones((X_all_data.shape[0],1))\n",
    "X_train = np.hstack((one_col_train, X_all_data))\n",
    "one_col_test = np.ones((X_single_test.shape[0],1))\n",
    "X_test = np.hstack((one_col_test, X_single_test))\n",
    "    \n",
    "w_best = get_coeff_ridge_normaleq(X_train, y_all_data, alpha_best)\n",
    "y_single_test = X_test.dot(w_best)[0][0]\n",
    "print(\"The predicted price is: {}, when alpha = {}\".format(y_single_test, alpha_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted price is: 25.62735711812161, when alpha = 1.023292992280754\n"
     ]
    }
   ],
   "source": [
    "alpha_best = 1.023292992280754\n",
    "w_best = get_coeff_ridge_normaleq(X_train, y_all_data, alpha_best)\n",
    "y_single_test = X_test.dot(w_best)[0][0]\n",
    "print(\"The predicted price is: {}, when alpha = {}\".format(y_single_test, alpha_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
